---
title: The role of metadata in the Balanced Corpus of Contemporary Written Japanese in the analysis of register
subtitle:
shorttitle: Role of metadata in the BCCWJ
author: Bor Hodošček
email: bor.hodoscek@gmail.com
author-comment: This work was sponsored under JSPS Foreign Post-Doc Fellowship grant \#P13303.
author-affiliation: National Institute for Japanese Language and Linguistics
author2: Hilofumi Yamamoto
author2-affiliation: Tokyo Institute of Technology
email2: yamagen@ryu.titech.ac.jp
date: \today
abstract: |
  This study proposes to evaluate the discriminatory power of the metadata contained within the Balanced Corpus of Contemporary Written Japanese (BCCWJ) for the modeling of linguistic variation (register).
  The available metadata is analyzed into several categories thought to influence register (NDC category hierarchy, gender, topic, media, etc.), which are then used to partition the documents within the corpus along different category groupings.
  The resulting similarity scores between the linguistic features of the
  category groupings reveal the relationships between--as well as the constraints and gaps within--the metadata, which is essential information for the reliable measurement of differences in register. (99 words)

  This study proposes to evaluate the discriminatory power of the metadata contained within the Balanced Corpus of Contemporary Written Japanese (BCCWJ) for the modeling of linguistic variation (register).
  We structure the metadata, which contains different types of facts (genre label, author, year, etc.), into a graph, and measure the linguistic similarities between different subsets of the corpus set along disjoint parts of the metadata graph structure.
  The resulting similarity scores will reveal the relationships between--as well as the constraints and gaps within--the metadata, which can inform the construction of an ontology of corpus metadata usable for register studies.

  Construction of an ontology of genres from media labels in the Balanced Corpus of Contemporary Written Japanese.
  By making the calculation of media labels tractable, we take a first step towards an empirically motivated evaluation of the validity of the labels in representing genre and register differences.
  Construction of a database for profiling the register of a word or collocation.
  Merging of lexical and textual meta-information in the analysis of both words and metadata.
  Focus on integrating meta-information that is indirectly associated with register.
  The metadata within the BCCWJ encodes knowledge (contextual factors or other facts) at the corpus, document, paragraph, and sentence levels.
  This metadata, which encodes metadata at several linguistic levels, can be expressed in terms of both a relational database or graph structure.
  The present research focuses on modeling the metadata in a graph structure with an eye towards eventually standardizing on an ontological representation, namely OWL.
  However, it can be advantageous to be able to reason over the metadata structure when conducting analyses on linguistic data that depend on the metadata for inference.
  One common example of the use of metadata in linguistic studies is the use of metadata on genre or register which can provide contrasting subsets of the corpus for analysis.
  By encoding the metadata available and giving it structure, we also open up the metadata to the use of reasoning engines that should make it possible to infer or deduce information previously hard to extract or conceptualize.
  Finally, the present research will look at computing the linguistic similarities between the different levels of the metadata as a way to quantify the appropriateness of metadata for contrastive studies of language data.
  In contrast to studies that use corpora for lexical ontology construction, which are plentiful, studies that also treat the metadata of the corpus as an ontology are more rare.
  Information integration between labels that differ in what subsets of genre or register (contextual instantiations?) differences they describe.


tags: [register, genre, style, ontology, OWL, RDF, Japanese, corpus, BCCWJ]
publisher: To be presented at the 19th Symposium of the Association for Databases on the Humanities
columns: 2
---

# Introduction

1.  I am working on the topic of X
    2.  because I want to find out Y
        3.  so that I can help others understand Z

-   what is the question?
-   motivation of the question
-   overview of previous work
-   formulation of hypotheses

Numerous studies on the topic of linguistic variation have been conducted under the monicer of style, genre, register, text type, domain, and dialect, to describe the sociolinguistic, functional, and ... variation observable in language [@eckert2001style; @Biber1995; @eckert2001styleIrvine; @@Lee2001].

Register and genre distinction as language and society distinction. Biber: functional association between linguistic forms and situations of use results in the systematic patterns of register variation.

In the present work, we offer the following tentative interpretations of register, genre, style, text type, and domain [Also see @Lee2001]:

-   Register:
    A variety of language associated with the specific situation of use. Example: register of written academic Japanese; classroom conversation

-   Genre:
    A category of language defined by a community, or associated with expected rhetorical structure and themes. Example: genre of Japanese research articles; crime novels

-   Style:
    Variations in language associated with an individual's 'unique' uses of language. Example: sensationalist style; vague written style

-   Text type:
    A grouping of texts based purely on linguistic features.
    Example: informational text type

-   Domain:
    Text devoted to a single topic or a small set of related topics, often inside one genre. Example: domain of computational linguistics

-   Dialect:
    [@eckert2001styleIrvine]



Different approaches to modeling language variation have been developed, mirroring the needs and interests of the following fields: socio-linguistics, historical linguistics, dialect research, NLP, text classification, authorship identification ...

Our purpose here is to define register with respect to corpus, topic, metadata, and situational types.

In this paper we mostly focus on the corpus as the tool for measuring linguistic variation.

The BCCWJ is the most comprehensive resource for investigating register variation in contemporary written Japanese, though it is not without problems when used for this kind of purpose (cf. [@tanomura2012; @tanomura2013]).


$\Delta$

-   The most notable approach of this category has been proposed by Burrows (2002) under the name ‘Delta’. First, this method calculates the z- distributions of a set of function words (originally, the 150 most frequent words). Then, for each document, the deviation of each word frequency from the norm is calculated in terms of z-score, roughly indicating whether it is used more (positive z-score) or less (negative z-score) times than the average. Finally, the Delta measure indicating the difference between a set of (training) texts written by the same author and an unknown text is the mean of the absolute differences between the z-scores for the entire function word set in the training texts and the corresponding z-scores of the unknown text. The smaller Delta measure, the greater stylistic similarity between the unknown text and the candidate author. It has been demonstrated that it is a very effective attribution method for texts of at least 1,500 words. For shorter texts the accuracy drops according to length. However, even for quite short texts, the correct author was usually included in the first five positions of the ranked authors which provides a means for reducing the set of candidate authors. [@stamatatos2009survey]


Kin.

(Find psychological backing for the assertion that the human language system responsible for functional/closed word categories stops developing after a certain age and can act as a fingerprint.)


-   Stanford lab: [p. 24]
    -   Involuntary signs: this is certanly what MFW and LATs are. But are they just that? Because, clearly, there is a problem with earlobes and fingernails: good as they might be at identifying the author of a painting, they are worthless at explaining its meaning. In fact, they are good at the one because they are bad at the other: it’s only because “trifles” have no structural function, that authors let go and “write unintentionally, without realizing it” – thereby betraying themselves. If those words were important, they would be more careful.

        There is something paradoxical in these traits that classify so well, and explain so little.  Especially so in our case: because, after all, MFW and LATs were in at least one respect the very opposite of earlobes and fingernails: instead of being rare and peripheral details, they were so frequent as to be almost ubiquitous. And how could such pervasive traits tell us nothing about the structure of genre? It was possible, of course, that it was all our fault; that, although we had managed to isolate the data, and were probably the first to “see” them, we just didn’t know how to make sense of them. Possible; and we are ready to place our data at the disposal of others, who may obtain better results.

        But there is also a simpler explanation: namely, that _these features which are so effective at differentiating genres, and so entwined with their overall texture – these features cannot offer new insights into structure, because they aren’t independent traits, but mere consequences of higher-order choices_. Do you want to write a story where each and every room may be full of surprises? Then locative prepositions, articles and verbs in the past tense are bound to follow. They are the effects of the chosen narrative structure. And, yes, once Docuscope and MFW foreground them, making us fully aware of their presence, our knowledge is analytically enriched: we “see” the space of the gothic, or the link between action verbs and objects (highlighted by the frequency of articles), with much greater clarity. But, for the time being, the gain seems to be comparative more than qualitative: greater clarity, rather than clarity of a different type.


### Misc

-   [@kilgarriff2001comparing] : ...

-   [@gries2009n]

-   [@gries2008dispersions; @th2009dispersions; @lijffijt2008correction]

-   [@gries2009bigrams]

-   [@a2013arXiv1309.3323U]

-   [@Forsyth06022013]

# Materials

## BCCWJ



# Methods

## Language-External Criteria

### Corpus Metadata as Ontology

There are many types of metadata: (From [Wikipedia](http://en.wikipedia.org/wiki/Metadata))

-   **Guide metadata** are used to help humans find specific items and are usually expressed as a set of keywords in a natural language. The NDC, C-Code, Magazine codes, Yahoo! hierarchies correspond to this type of metadata.
    On the other hand, Diet, Whitepapers are classified according to an informal structure that records different kinds of situational characteristics.
-   Metadata schema can be hierarchical in nature where relationships exist between metadata elements and elements are nested so that parent-child relationships exist between the elements. An example of a hierarchical metadata schema is the IEEE LOM schema where metadata elements may belong to a parent metadata element. Metadata schema can also be one-dimensional, or linear, where each element is completely discrete from other elements and classified according to one dimension only. An example of a linear metadata schema is Dublin Core schema which is one dimensional^[How is this even possible? OWL/RDF should imply a hierarchy.]. Metadata schema are often two dimensional, or planar, where each element is completely discrete from other elements but classified according to two orthogonal dimensions.
    -   Look at [国立国会図書館ダブリンコアメタデータ記述（DC-NDL2011年12月版）](http://www.ndl.go.jp/jp/aboutus/standards/meta.html#dcndl201112) for a more comprehensive Dublin Core-like schema.
-   **Granularity** (a central point we want to make: what is the right granularity for the description of register?): The degree to which the data or metadata are structured is referred to as their granularity. Metadata with a high granularity allow for deeper structured information and enable greater levels of technical manipulation however, a lower level of granularity means that metadata can be created for considerably lower costs but will not provide as detailed information. The major impact of granularity is not only on creation and capture, but moreover on maintenance (bottom-up less of a problem). As soon as the metadata structures get outdated, the access to the referred data will get outdated. Hence granularity shall take into account the effort to create as well as the effort to maintain.
-   The Simple Dublin Core Metadata Element Set (DCMES) consists of 15 metadata elements: (1-D??)
    -   Title
    -   Creator
    -   Subject
    -   Description
    -   Publisher
    -   Contributor
    -   Date
    -   Type
    -   Format
    -   Identifier
    -   Source
    -   Language
    -   Relation
    -   Coverage
    -   Rights
-   [Functional Requirements for Bibliographic Records](http://www.ifla.org/publications/functional-requirements-for-bibliographic-records) might be worth a look.


Before examining linguistic features that measure register, I ...

Metadata contained in corpora can often be represented in a hierarchy.
For example, the text categories in the Brown Corpus can be represented as a three-level hierarchy^[More information on the Brown Corpus, as well as the full list of categories is available at <http://icame.uib.no/brown/bcm.html>.].


Table: A sample of the text categories of the Brown Corpus in 1961.

**Top Category** **Text Category** **Subcategory**
---------------- ----------------- -----------------------
PRESS            Reportage         Political
                                   Sports
                                   ...
                                   Cultural
                 Editorial         Institutional Daily
                                   Personal
                                   Letters to the Editor
                 Reviews           ...
RELIGION         (RELIGION)        Books
                                   Periodicals
                                   Tracts
LEARNED          (LEARNED)         Natural Sciences
                                   Medicine
                                   ...
...              ...               ...


[@wu2010fine]

In reality, a strict hierarchy is not able to capture (is a clumsy way of capturing) all the structure offered in many corpora.
This is, for example, because the branching factor in the hierarchy is often a combination of topic, register, and genre differences, which obviously interact at different levels and, in essence, require hierarchies of their own.
Eventually, the realization of an ontology of variation within and between texts would allow a systematic encoding of the metadata inherent in a document.
Instead, the relationships between the texts in a corpus and the meta-information available can only ideally be modeled as an ontology.

-   "if one performs a corpus-linguistic quantitative analysis of phenomenon X in a corpus using the statistical parameter P, (i) from a descriptive perspective, which degree of variability of P was observed in one’s data set and how do we quantify it? And, how do the present results concerning P compare to those of other studies? These questions inevitably lead to the next: (ii) how homogeneous is the corpus that was used for the study of phenomenon X? And, (iii) from an exploratory, bottom-up perspective, how can one identify (some of) the (most) relevant sources of the observed variability of P? If we rephrase that from a hypothesis-testing, top-down perspective: are the variables A, B, C, responsible for a significant proportion of P’s variability?" [@gries2006exploring, p. 113]
-   "Beginning with data used by Schlüter to address the issue of reliability of corpus findings, I noted that discussing the reliability of any particular statistic is difficult without a precise indication of both the internal and external variability of the data sets in question. From the fact that corpus homogeneity/variability involves making decisions concerning the parameter of interest as well as the desired level of granularity, I illustrated several ways to address these issues. Specifically, I proposed ways to determine, (i) how large the differences between results of different corpus parts are on each level, and (ii) which level introduces the largest differences between results. I proposed that the results of both of these issues constitute important quantitative information in their own right as well as objectively identify from the data ideal starting points for subsequent analysis in a bottom-up fashion. That is, the proposed methods involve splitting up one’s corpus on the basis of parameters of different degrees of granularity as well as splitting it up into parts and then using permutational and/or bootstrapping approaches to investigate central tendencies, frequency distributions of differences of mutually-exclusive corpus parts and homogeneous groups in one’s data. Finally, I proposed to quantify corpus homogeneity using multivariate exploratory data analysis techniques." [@gries2006exploring, p. 144--145]
-   "I submit that this kind of approach – irrespective of its exact method of implementation – has a variety of advantages over much previous corpus-based work in general as well as work on corpus homogeneity. First, it provides an indication of the dispersion of the statistical parameter that is reported. This result is more informative and robust than any single confidence interval, standard error, etc. and so enormously enhances the descriptive adequacy of the results, as well as the potential to generalise.

    Secondly, it allows one to apply flexibly the methods to parameters of all levels of statistical sophistication: simple frequencies, percentages or conditional probabilities, association measures, regression weights, etc.

    Thirdly, it allows for objectively identifying (i) corpus parts exhibiting noteworthy results on one’s parameter of interest (e.g., outliers within, or tails of, distributions of differences); and (ii) the major determinants of variability within whatever set of corpus divisions appear sensible (e.g., register or sub-register divisions, lexical elements involved in grammatical constructions (cf. the particles above) or any others).  Given the potential wealth of results from simultaneous comparisons, this aspect of these methods should be especially welcome to researchers involved in, and underscoring the need of studying, register differences. It is of course possible, however, that a rigorous bottom-up identification will show that some of the traditional distinctions do not correspond to the levels of granularity where most variability can be found.  For example, Gries (forthcoming) shows that the speaking vs. writing distinction Newman and Rice argue for in recent work (see Newman and Rice, forthcoming) yields quantitatively different results, but not qualitatively different theoretical conclusions. Similarly, a considerable body of work conducted in the QLVL Research Group at KU Leuven has been concerned with documenting lectal variation such as, for example, the differences between Belgian and Netherlandic Dutch. However, this approach is really just an instance of the kind of (well-educated) guessing at which level of granularity (variety) and between which levels of this level (Dutch vs. Belgian Dutch) effects are to be found. True, many results showed that this distinction sometimes does explain a considerable amount of variance, but a more exhaustive bottom-up approach may well show that _dividing the corpus data differently may reveal factors with a higher degree of explanatory power or effect size and/or theoretical relevance_ (see Gries, forthcoming, for discussion).

    Fourthly, the approach proposed here is based on _effect sizes_ and, thus, avoids the shortcomings of null-hypothesis significance testing (e.g., the-sample-size-is-always-big-enough problem). However, it can also be used to approximate p-values without making/violating any distributional assumptions. Finally, it allows us to assess the tricky issue of _corpus homogeneity with respect to two parameters_ that are usually not even motivated explicitly: the _degree of granularity_, which is almost exclusively implemented by simply taking files, and the _parameter of interest_, which is usually implemented by using word frequencies. The method proposed here allows us to use any degree of granularity and one’s parameter of interest. Also, it is possible to both identify the parts of one’s corpus which are most/least representative of the whole corpus as well as quantify the homogeneity of one’s own corpus for comparison with other corpora." [@gries2006exploring, p. 146--147]

### BCCWJ

Focus on meta-information ("data about data").

The BCCWJ carries a variety of meta-information including the author(s)' name, sex, date of birth, as well as the publishing date, publisher name, but most importantly for our purposes, up to 4 genre-related labels per document.
These genre labels represent different conceptualizations of "genre" depending on the sub-corpus of interest.

NDC, C-Code is a domain ontology, and a formal taxonomy ("taxonomy is a hierarchical structure for the classification or organization of data").
WordNet is a general ontology, a thesaurus.
Yahoo! Blogs and Q&A are both informal (or formal??? - are users aware of the hierarchy when posting?) taxonomies.
Guided, hierarchical metadata.


[Open Directory Project](http://www.dmoz.org/docs/en/about.html)

[International Patent Classification (IPC)](http://www.wipo.int/classifications/ipc/en/)



**Corpus**          **Metadata type**         **Topic**        **Register**
------------------  ------------------------  ---------------- --------------------
Books               NDC....                   Yes              Not really
Yahoo! Blogs        Topic categories (tags)   Yes              No
...


-   Some comments on the reliability of some parts of the BCCWJ: [@tanomura2012; @tanomura2013]

@wu2010fine:

-   "In this paper, we have evaluated structural learning approaches to genre classification using several different genre distance measures. Although we were able to improve on non-structural approaches for the Brown corpus, we found it hard to improve over flat SVMs on other corpora. As potential reasons for this negative result, we suggest that current _genre hierarchies are either not of sufficient depth or are visually or distributionally imbalanced._ We think further investigation into the relationship between hierarchy balance and structural learning is warranted. Further investigation is also needed into the appropriateness of n-gram features for genre identification as well as good measures of genre distance. In the future, an important task would be the _refinement or unsupervised generation of new hierarchies, using information theoretic or data-driven approaches_. For a full assessment of hierarchical learning for genre classification, the field of genre studies needs a testbed similar to the Reuters or 20 Newsgroups datasets used in topic-based IR with a balanced genre hierarchy and a representative corpus of reliably annotated webpages. With regard to algorithms, we are also interested in other formulations for structural SVMs and their large-scale implementation as well as the combination of different distance measures, for example in ensemble learning." [@wu2010fine, p. 757]
    -   **visually or distributionally imbalanced**: "To measure the degree of balance of a tree, we introduce two tree balance scores based on entropy. First, for both measures we extend all branches to the maximum depth of the tree. Then level by level we calculate an entropy score, either according to how many tree nodes at the next level belong to a node at this level (denoted as vb: visual balance), or according to how many end level documents belong to a node at this level (denoted as db: distribution balance). To make trees with different numbers of internal nodes and leaves more comparable, the entropy score at each level is normalized by the maximal entropy achieved by a tree with uniform distribution of nodes/documents, which is simply −log(1/N), where N denotes the number of nodes at the corresponding level. Finally, the entropy scores for all levels are averaged. It can be shown that any perfect N-ary tree will have the largest visual balance score of 1. If in addition its nodes at each level contain the same number of documents, the distribution balance score will reach the maximum, too. " (p. 755)
        -   Could be used to position the study _vis-a-vis_ the Brown Corpus, as well as give an indication of the structural differences and variability between different parts of the BCCWJ.


@park2008multi

-   "In many applications, there are explicit constraints that must hold between the labels. For example, in the context of hierarchical classification, the presence of one label in the hierarchy often also implies the presence of all its ancestors.  This situation can be modeled by a subset constraint, which specifies that whenever label λi is predicted as relevant, we must also predict λj . Similarly, one can imagine exclusion constraints specifying that two labels λi and λj cannot be relevant at the same time. A typical example of this case would be if the possible set of labels contains labels of several orthogonal dimensions, each having a set of mutually exclusive labels." [p. 2]


## Language-Internal Criteria

The relations between different methods of extracting words and collocations from documents are diagrammed in ... .

-   operationalization of variables: corpus homogenity/variability/metadata correlations
-   choice of method (e.g, diachronic vs. _synchronic data_, _tagged_ vs. untagged _data_, etc.)
-   source of data
-   retrieval algorithm or syntax

    $tf\text{-}idf$
    -   idf is an important normalization concept when we are dealing with samples from spaces of register
    -   why tf-idf for collocations? because we can define it for arbitrary numbers of n, while most other statistical collocation measures are fixed to n=2 (find Croatian paper)
    -   "For general texts, one option suitable for some texts may not suitable for others. In contrast, it is easier to select options based on properties of short texts. For example, because in a short text words are generally distinct, Yu et al. (2012) show that the SVM models obtained by _binary and TF-IDF feature representations give similar performance_." [@yuproduct]
    -   [@gebre2013improving]  -- Native Language Identification

-   software that was used
-   data filtering/annotation (how were false hits identified, what did you do to guarantee objective coding procedures? how did you annotate your data?)
-   choice of statistical test


According to [@Biber2009, p. 40 (Table 2.1); Finegan and Biber's taxonomy of situations!], the above terms differ in their configurations of at least the following situational factors: ....
participants,
relations among participants,
channel,
production circumstances,
setting,
communicative purposes, and
topic (cf. genre as ontology described in @...).


What kind of metainformation on context can we actually infer from corpora -- or even tag ourselves?
How much does not knowing the cultural context hurt us when analyzing documents?

Media/metadata as ontology graph, using weighted links between different parts within a media and between media.
Consider RDF linked data output in system. How should this be positioned relative to the JSON API? JSON-LD and triple- vs entity-centric data model. Efficient graph structure if doing home-grown (intermediate?) data store. How to efficiently model the metadata and words in the BCCWJ -- do we want to keep information on which sentence a word occurs in as an edge? DRAW!!!!!!
Minimal discriminating difference network for final display.

Media metainformation as ontology. Which parts/links are implied in the current metadata, and which can be extracted/inferred from the language data? Use [https://github.com/phillord/tawny-owl] to programmatically make an ontology on the fly. What kind of inferences can we do on this data with standard ontology (OWL) reasoners, and would core.logic or datalog help? Disjoint classes in OWL. collection `contains` articles. [http://ontogenesis.knowledgeblog.org/1401]
What kind of automatic inferences can we make on the data with for example simple linear regression (refer to graph/LR paper -- inductive and deductive reasoning).
The purpose of the ontology is to make the calculation of metainformation tractable, and to discover where the metainformation structure is lacking by way of comparisons of language data.

OWL specifies general logical constraints.
OWL-DL seems the most reasonable implementation.

Read [http://shirky.com/writings/ontology_overrated.html]:

-   "What's being optimized is number of books on the shelf. That's what the categorization scheme is categorizing." -- referring to the DDS.
-   hierarchy vs. link structure---a true dichotomy?
-   Should we expect ontological classificaton to help here?
-   You can also turn that list around. You can say "Here are some characteristics where ontological classification doesn't work well":
    -   Domain
        -   Large corpus
        -   No formal categories
        -   Unstable entities
        -   Unrestricted entities
        -   No clear edges
    -   Participants
        -   Uncoordinated users
        -   Amateur users
        -   Naive catalogers
        -   No Authority
-   If you've got a large, ill-defined corpus, if you've got naive users, if your catalogers aren't expert, if there's no one to say authoritatively what's going on, then ontology is going to be a bad strategy.
-   **Signal Loss from Expression** - "The signal loss in traditional categorization schemes comes from compressing things into a restricted number of categories. With tagging, when there is signal loss, it comes from people not having any commonality in talking about things. The loss is from the multiplicity of points of view, rather than from compression around a single point of view. But in a world where enough points of view are likely to provide some commonality, the aggregate signal loss falls with scale in tagging systems, while it grows with scale in systems with single points of view."
    -   How much signal loss is there between different levels of the hierarchy?
-   "There isn't in fact a binary condition of a tag that can or cannot survive any kind of long-term examination. " cf. the genre as formal ontology paper [@garbacz2006outline]

-   http://www.w3.org/Submission/SWRL/
-   http://owlapi.sourceforge.net/
-   http://blog.neo4j.org/2013/08/and-now-for-something-completely.html
-   [@garbacz2006outline]
    -   [@orlikowski1994genre]
    -   [@yates1992genres]
    -   [@yates1999explicit]

-   Cognitive evidence for this hypothesis is offered in @Mcdonald01testingthe.
-   "[The Distributional Hypothesis](http://en.wikipedia.org/wiki/Distributional_semantics) in linguistics is the theory that words that occur in the same contexts tend to have similar meanings.[1] The underlying idea that "a word is characterized by the company it keeps" was popularized by Firth."
-   "Different kinds of similarities can be extracted depending on which type of distributional information is used to collect the vectors: topical similarities can be extracted by populating the vectors with information on which text regions the linguistic items occur in; paradigmatic similarities can be extracted by populating the vectors with information on which other linguistic items the items co-occur with. Note that the latter type of vectors can also be used to extract syntagmatic similarities by looking at the individual vector components."
-   Distributional semantic models differ primarily with respect to the following parameters:
    -   Context type (text regions vs. linguistic items)
    -   Context window (size, extension, etc.)
    -   Frequency weighting (e.g. Entropy, Pointwise mutual information, etc.)
    -   Dimension reduction (e.g. Random indexing, Singular value decomposition, etc.)
    -   Similarity measure (e.g. Cosine similarity, Minkowski distance, etc.)
-   Distributional semantic models that use linguistic items as context have also been referred to as word space models

### Word Weighting

Operationalization of variables/retrieval algorithm.

-   Why tf-idf? Because it models the context dispersion as well as frequency of a word.
    -   low tf-idf -> word appears in most contexts
-   When does the tfidf measure fail and why?
-   What is the midrank (if the lowrank is grammatical/function words and the highrank is topical/proper noun words)?
-   What is the meaning of the weight? Can the function-content scale capture/explain the real data well? Is it the only scale we want/are interested in?
-   What is the Query in the TF-IDF model when we have no relevancy information and an actual query -- i.e. how does this even make sense/work?
-   Termhood -- Basili et al 2001

### Distance Functions

$\Delta$

### Word Register Weighting

Lexical domains. Lexical profiling?

Semantic similarity vector spaces. What is the connection between the distance between two words and the different senses of a word? Can we link both the different senses of a word and the vector space, so that we would be able to know with which sense of a word another word cooccurs with? Can the edge between two words in any way capture the sense of the words involved/connected by the edge?

What is the link between the sparse-dense scale and the diffuse-concentrated scale, as well as the function-content scale (of TF-IDF)? When splitting words into low-middle-high classes, compare word classifications between different contexts to see if they move. Compare two concepts at once. $\alpha(A,B) = P(A|B) \cdot P(B|A)$ -> $P(A|B)$ (actually read [http://en.wikipedia.org/wiki/Mutual_information]) is the ratio of words in a span of $A$ that moved to the same span in $B$, while $P(B|A)$ is the ratio of words in a span of $B$ that moved to the same span in $A$ (==Mutual Information). Is this a novel/good way of measuring context similarity? We can even use sentences as the unit of IDF, where each document would then have a unique TF-IDF distribution, and we could repeat the same process as above. In effect, we would then get a contextual distribution over each word.
Compare the contextual distributions of a word between two contextual slices.

Compare with [@rybicki2011deeper]:

-   "Each analysis was made for the top 50-5000 most frequent words in the corpus - but then the 50 most frequent words would be omitted and the next 50-5000 words taken for analysis; then the first 100 most frequent words would be omitted, and so on. This was done with a single R script written by Eder; the script produced word frequency tables, calculated Delta and produced "heatmap" graphs of Delta's success rate for each of the frequency list intervals, showing the best combinations of initial word position in wordlist and size of window, including variations of pronoun deletion and culling parameters. Thus, in the resulting heatmap graphs, the horizontal axis presents the size of each wordlist used for one set of Delta calculations; the vertical axis shows how many of the most frequent words were omitted. Each of the runs of the script produced an average of ca. 3000 Delta iterations." [p. 1]
-   upon reflection, while the heatmap viz does not fit this paper, we could smartly order each sample into a rank (highlighting where indexes begin) to make a corellelogram (doh!)


@faatz2002ontology

-   TODO

sub-groop discovery ala MIDOS [@wrobel1997algorithm]

-   "While classification rule learning is an approach to predictive induction (or supervised learning), aimed at constructing a set of rules to be used for classification and/or prediction, association rule learning is a form of descriptive induction (non-classificatory induction or unsupervised learning), aimed at the discovery of individual rules which define interesting patterns in data." [@langohr2013contrasting, p. 153-154]
-   Like MIDOS, we are looking for subgroups that have unusual distributional characteristics with respect to the entire population, but we are also further interested in unusual distributions with respect to the parent subgroup.
-   We should modify the evaluation function to contain some measure of distributional similarity and not just median differences.
    -   boosting on tf and df(?)
-   [Binary decision diagram](https://en.wikipedia.org/wiki/Binary_decision_diagram), [Zero-suppressed decision diagram](https://en.wikipedia.org/wiki/Zero-suppressed_decision_diagram), [JDD, a pure Java BDD and Z-BDD library](http://javaddlib.sourceforge.net/jdd/)

The accuracy of a propositional classification rule of the form Class ← Cond is equal to the conditional probability of class Class, given that condition Cond is satisfied.
Using the Laplace estimate: $Acc(Class \leftarrow Cond) = \frac{n(Class.Cond) + 1}{n(Cond) + k}$, where $k$ is the number of classes.

\vspace*{1em}
\begin{algorithm}[H]
   \label{alg:MIDOS}
   \caption{MIDOS algorithm (\cite{wrobel1997algorithm}, p. 6; also see Slides at \url{http://www-ai.ijs.si/~ilpnet/stefan/slide30.htm}).}
   \SetAlgoLined
   \RestyleAlgo{algoruled}
   \DontPrintSemicolon
Let $Q := r_0(V_1,...,V_{a(r_o)}), H := \emptyset .$\;
\While{$Q \ne \emptyset$}{
    select a subset $C$ from $Q$ according to \textit{search strategy}\;
    let $\rho(C) := \{\rho(h) | h \in C\}$\;
    test each $h \in \rho(C)$ on $D$ (compute $d(h, D)$ using sampling)\;
        \uIf{$d(h, D) = 0$}{prune}
        \uElseIf{$d_{max}(h,D) < min_{h' \in H} d(h', D)$}{prune}
        \uElse{
          \uIf{$d(h, D) > min_{h' \in H} d(h',D)$}{remove the bottommost element of $H$ and add $h$}
          add $h$ to $Q$
        }
}
  \vspace*{1em}
\end{algorithm}
\vspace*{1em}

-   dispersion as the measure of contextual binding of words, and a set of dispersions as a grouping  of context.
-   by looking at subgroups and taking into account interactions between categories, we can sometimes avoid incorrect conjectures cf. Simpson's paradox.
-   bayesian networks between labels can show how their relations change depending on subgroup

<!-- FIXME look at books for formatting help -->

\begin{equation}
    \text{tf-idf}(w_i)
    \begin{dcases}
        \ge \text{tf-idf}_{+\sigma}                                 & \rightarrow \text{high}\\
          > \text{tf-idf}_{-\sigma} \land < \text{tf-idf}_{+\sigma} & \rightarrow \text{mid}\\
        \le \text{tf-idf}_{-\sigma}                                 & \rightarrow \text{low}
    \end{dcases}
\end{equation}

\vspace*{1em}
\begin{algorithm}[H]
   \label{alg:resampling}
   \caption{Bootstrap resampling (or exhaustive permutation) of documents to get confidence intervals on word weights.}
   \SetAlgoLined
   \RestyleAlgo{algoruled}
   \DontPrintSemicolon

   $N \leftarrow$ number of documents in contextual slice $C$\;
   \For{each document $D$ in contextual slice $C$}{
     Some smart way of resampling the documents to adjust their number and make them of comparable lengths.\;
  }

   \vspace*{1em}
\end{algorithm}
\vspace*{1em}

$\delta(c, \bar{c}) = ...$

\vspace*{1em}
\begin{algorithm}[H]
   \label{alg:comparison}
   \caption{Calculate distance metrics between all contrasting context slices/spaces. Should this be between or within levels (complicated if we include sentence-level tags)?}
   \SetAlgoLined
   \RestyleAlgo{algoruled}
   \DontPrintSemicolon
  Define sets of context slices to compare \nllabel{A:grid}\;
  \For{each context slice $c \in C$}{
    Compute the $\text{tf-idf}(t, \text{df}_c)$ for $\forall t \in T$ \;
    \For{each metric $M$}{
      Initialize empty matrix $\Delta_M$ using metric $M$ \;
      $\Delta_M \leftarrow$
      \For{each context slice $\bar{c} \ne c \land \bar{c} \in C$}{
        $\delta(c, \bar{c})$ \;

        %Hold—out specific samples \nllabel{A:resample} \;

        %[Optional] Pre—process the data\;
      }
    }
  }
  \vspace*{1em}
\end{algorithm}
\vspace*{1em}

\vspace*{1em}
\begin{algorithm}[H]
   \label{alg:tf-idf-comparison}
   \caption{Comparing overlap between context and tf-idf category.}
   \SetAlgoLined
   \RestyleAlgo{algoruled}
   \DontPrintSemicolon

   $A \cap B$ \;
   $A \setminus B$ \;
   $B \setminus A$ \;

  \vspace*{1em}
\end{algorithm}
\vspace*{1em}

![Comparing the distribution of tf-idf categories between context slices. The idf corresponds to the contextual slice frequency (csf). Propose a variant of the tf-idf measure that captures to some extent the distributional properties of words. tf-idf as scale. When comparing two sets of documents, the difference between the vocabulary scales in the two sets is a measure of the difference in the registers. Difference between distributions -- what algorithm to use? Differences between positions of words on scale, direction of difference matters!](images/register-database-tf-idf-comparison.pdf)


Is it possible to compare between TF-IDF weights in a diachronic analysis? Or even a synchronic/variational one? If the context space corresponds to the N, should we split the IDF portion per (coarse) context or take the big N of the total (estimated/known) language space? Specify the relationship between context, N, and query. If N is infinite than every word has the same weight(?), so do not take a big N as an ideal number. A context-sensitive IDF is more important.

We want to uncover words with extreme or interesting distributional tendencies. Intra- and inter-media differences are critical for modeling the topic and register dependence of words. Each of these dependencies could be modeled on a scale. How do these dependencies grow/evolve with the increase of data vis-a-vis the expansion (or non-expansion) of context?



# Results

# Discussion

-   implications of the findings for your hypothesis
-   implications of the findings for the research area

# Conclusion

To be written.

# Future Work

Examine relation between computed similarities to those of the text-external criteria of _katarikake-sei_ [cf. @Forsyth06022013].
